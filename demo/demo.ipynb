{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34c98e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import final\n",
    "import nltk\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # for debugging\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    " \n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "sentenc_model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentenc_model_name)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the BART modules from huggingface/transformers\n",
    "from transformers import BartTokenizerFast, BartForConditionalGeneration\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "def PrepareData():\n",
    "    # fns = glob.glob('./yale_dataset/*/*/*transcript.txt')\n",
    "    fns = ['./yale_dataset\\\\african-american-studies\\\\afam-162\\\\lecture-16_transcript.txt',\n",
    "    './yale_dataset\\\\biomedical-engineering\\\\beng-100\\\\lecture-5_transcript.txt',\n",
    "    './yale_dataset\\\\political-science\\\\plsc-114\\\\lecture-22_transcript.txt']\n",
    "\n",
    "    # fns = ['./yale_dataset/african-american-studies/afam-162/lecture-1_transcript.txt']\n",
    "    cnt = 0\n",
    "    sent_list = []\n",
    "    doc_index_list = []\n",
    "    yale_doc_dict = {}\n",
    "\n",
    "    for fn in fns:\n",
    "        with open(fn ,'r', encoding='utf8') as f:\n",
    "            para = f.readlines()\n",
    "            if len(para)==0:\n",
    "                continue\n",
    "            para = para[0]\n",
    "            a_list = nltk.tokenize.sent_tokenize(para)\n",
    "            sent_list.extend(a_list)\n",
    "            \n",
    "            dirs = fn.split('\\\\')\n",
    "            department_name = dirs[-3]\n",
    "            course_name = dirs[-2]\n",
    "            lecture_name = dirs[-1].split('_')[0]\n",
    "            \n",
    "            info = {'department':department_name, 'course':course_name, 'lecture':lecture_name}\n",
    "\n",
    "            summa_fn = fn.split('_transcript.txt')[0]+'_overview.txt'\n",
    "            \n",
    "            with open(summa_fn, 'r', encoding='utf8') as f2:\n",
    "                lines = f2.readlines()\n",
    "                description = ''\n",
    "                if len(lines)>=2:\n",
    "                    description = lines[1]\n",
    "                yale_doc_dict[cnt]={'info':info, 'description': description,'transcript':para}\n",
    "        \n",
    "        doc_index_list.extend([cnt]*len(a_list))\n",
    "        cnt+=1\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"sents\":sent_list, \"docs\":doc_index_list}) \n",
    "    df.to_json(\"yale_data.json\")\n",
    "\n",
    "    with open('yale_doc_dict.json', 'w') as json_file:\n",
    "        json.dump(yale_doc_dict, json_file)\n",
    "\n",
    "    return df, yale_doc_dict\n",
    "\n",
    "# Create a Data Loader Class\n",
    "class YaleData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, doc_dict, device):\n",
    "        self.len = len(dataframe)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.dict_list = []\n",
    "\n",
    "        for index in range(dataframe.shape[0]):\n",
    "            if index%50==0:\n",
    "                print(f\"Processing Yale Data: {index}/{dataframe.shape[0]}\")\n",
    "\n",
    "            sentence = dataframe.iloc[index].sents\n",
    "            document = doc_dict[dataframe.iloc[index].docs]['transcript']\n",
    "\n",
    "            inputs = self.tokenizer.batch_encode_plus(\n",
    "                [sentence, document], \n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\",\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            ids = inputs['input_ids']\n",
    "\n",
    "            mask = inputs['attention_mask']\n",
    "\n",
    "            self.dict_list.append({\n",
    "                'sent_ids': torch.tensor(ids[0], dtype=torch.long, device=device),\n",
    "                'doc_ids': torch.tensor(ids[1], dtype=torch.long, device=device),\n",
    "                'sent_mask': torch.tensor(mask[0], dtype=torch.long, device=device),\n",
    "                'doc_mask': torch.tensor(mask[1], dtype=torch.long, device=device)\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dict_list[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# get mean pooling for sentence bert models \n",
    "# ref https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "# Note that different sentence transformer models may have different in_feature sizes\n",
    "class SentenceBertClass(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\", in_features=384):\n",
    "        super(SentenceBertClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(model_name)\n",
    "        self.pre_classifier = torch.nn.Linear(in_features*3, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 1)\n",
    "        self.classifierSigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sent_ids, doc_ids, sent_mask, doc_mask):\n",
    "\n",
    "        sent_output = self.l1(input_ids=sent_ids, attention_mask=sent_mask) \n",
    "        sentence_embeddings = mean_pooling(sent_output, sent_mask) \n",
    "\n",
    "        doc_output = self.l1(input_ids=doc_ids, attention_mask=doc_mask) \n",
    "        doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
    "\n",
    "        # elementwise product of sentence embs and doc embs\n",
    "        combined_features = sentence_embeddings * doc_embeddings  \n",
    "\n",
    "        # Concatenate input features and their elementwise product\n",
    "        concat_features = torch.cat((sentence_embeddings, doc_embeddings, combined_features), dim=1)   \n",
    "        \n",
    "        pooler = self.pre_classifier(concat_features) \n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.classifierSigmoid(output) \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def ValidateModelYale(model, testing_loader, test_df, doc_dict):\n",
    "    model.eval()\n",
    "\n",
    "    subject=[]\n",
    "    course= []\n",
    "    title=[]\n",
    "    description=[]\n",
    "    transcript=[]\n",
    "\n",
    "    summary = {}\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testing_loader, 0): \n",
    "            outputs = model(data['sent_ids'], data['doc_ids'], data['sent_mask'], data['doc_mask']) \n",
    "\n",
    "            if torch.count_nonzero(outputs > 0.5)!=0:\n",
    "                bias_arr = torch.where(outputs>0.5)[0].cpu().detach().numpy()\n",
    "                for bias in bias_arr:\n",
    "                    doc_id = test_df.loc[i*testing_loader.batch_size + bias, 'docs']\n",
    "                    key = doc_id\n",
    "                    if key not in summary:\n",
    "                        summary[key] = []\n",
    "                    \n",
    "                    summary[key].append(test_df.loc[i*testing_loader.batch_size + bias, 'sents'])\n",
    "                          \n",
    "    for key in summary:\n",
    "        subject.append(doc_dict[key]['info']['department'])\n",
    "        course.append(doc_dict[key]['info']['course'])\n",
    "        title.append(doc_dict[key]['info']['lecture'])\n",
    "        description.append(doc_dict[key]['description'])\n",
    "        transcript.append(' '.join((summary[key])))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({\"subject\":subject, \"course\":course, \"title\":title,\"description\":description,\"transcript\":transcript}) \n",
    "\n",
    "    return df\n",
    "def ExtractiveModel(yale_df, yale_doc_dict):\n",
    "    MAX_LEN = 512\n",
    "    VALID_BATCH_SIZE = 4\n",
    "\n",
    "    \n",
    "\n",
    "#     print( \"Yale shape\", yale_df.shape)\n",
    "\n",
    "    yale_set = YaleData(yale_df, tokenizer, MAX_LEN, yale_doc_dict, device)\n",
    "\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 0,\n",
    "                    }\n",
    "\n",
    "    yale_loader = DataLoader(yale_set, **test_params)\n",
    "\n",
    "    model = SentenceBertClass(model_name=sentenc_model_name)\n",
    "    model.to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"models/minilm_bal_exsum.pth\"))\n",
    "    summary_df = ValidateModelYale(model, yale_loader, yale_df, yale_doc_dict)\n",
    "    return summary_df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.description\n",
    "        self.ctext = self.data.transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def validate(tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "#             if _%100==0:\n",
    "#                 print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals\n",
    "\n",
    "def AbstractiveInference(df):\n",
    "    TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "    VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "    TRAIN_EPOCHS = 20        # number of epochs to train (default: 10)\n",
    "    VAL_EPOCHS = 1 \n",
    "    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    SEED = 42               # random seed (default: 42)\n",
    "    MAX_LEN = 512\n",
    "    SUMMARY_LEN = 150 \n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(SEED) # pytorch random seed\n",
    "    np.random.seed(SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Importing and Pre-Processing the domain data\n",
    "    # Selecting the needed columns only. \n",
    "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "    df = df[['transcript','description']]\n",
    "    df = df[(df['transcript'].str.len()>200) & (df['description'].str.len()>20)]\n",
    "    df.description = 'summarize: ' + df.description\n",
    "#     print(df.head())\n",
    "\n",
    "    val_dataset=df\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Loading model\n",
    "\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "    model.load_state_dict(torch.load(\"./models/abs_model\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "#     print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    \n",
    "    predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb5f8f",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2b09e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "yale_df, yale_doc_dict = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e540034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of the transcripts(words):  5988.666666666667\n"
     ]
    }
   ],
   "source": [
    "length_map = list(map(lambda x: len(x[1]['transcript'].split(' ')),yale_doc_dict.items()))\n",
    "print(\"Mean length of the transcripts(words): \", sum(length_map)/len(length_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b3e6c",
   "metadata": {},
   "source": [
    "## Extractive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd41389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yale Data: 0/721\n",
      "Processing Yale Data: 50/721\n",
      "Processing Yale Data: 100/721\n",
      "Processing Yale Data: 150/721\n",
      "Processing Yale Data: 200/721\n",
      "Processing Yale Data: 250/721\n",
      "Processing Yale Data: 300/721\n",
      "Processing Yale Data: 350/721\n",
      "Processing Yale Data: 400/721\n",
      "Processing Yale Data: 450/721\n",
      "Processing Yale Data: 500/721\n",
      "Processing Yale Data: 550/721\n",
      "Processing Yale Data: 600/721\n",
      "Processing Yale Data: 650/721\n",
      "Processing Yale Data: 700/721\n"
     ]
    }
   ],
   "source": [
    "summary_df = ExtractiveModel(yale_df, yale_doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e3d3f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of the transcripts(words):  2750.6666666666665\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of the transcripts(words): \", summary_df[\"transcript\"].apply(lambda x: len(x.split(' '))).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c678d5",
   "metadata": {},
   "source": [
    "## Abstractive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "741d8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\envs\\CSCI544\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_df = AbstractiveInference(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "063764b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['Generated Text'] = final_df['Generated Text'].str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7e43b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>Actual Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: In this lecture, Professor Holloway revisits Malcolm Xâs life in order to offer a richer portrait of the black leader than his predecessors. A critical analysis of Malcolm X reveals that, in the absence of a strong social context, one of the most important and ultimately most pernicious debates in the Nation of Islam (NOI), is predicated on ideas of how race can be replaced by a notion of accountability. In the final year of his life, prior to his assassination by NOI members in 1965, Malcolm X makes a religious journey to Mecca, rejects his prior views on race, starts with the Organization of Afro-American Unity, and adopes the philosophy of white supremacist violence</td>\n",
       "      <td>summarize: In this lecture, Professor Holloway revisits Malcolm X’s life in order to offer a more nuanced interpretation of the black leader than is traditionally taught. Professor Holloway links Malcolm X to a tradition of black intellectuals and political activists like Booker T. Washington, Marcus Garvey, and Robert Williams, and he explores the philosophy of the Nation of Islam (NOI), the organization for which Malcolm X is the national spokesman before his split with Elijah Muhammad in 1964. In the final year of his life, prior to his assassination by NOI members in 1965, Malcolm X makes a religious journey to Mecca, rejects his prior views on race, starts the Organization of Afro-American Unity, and adopts the name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: Professor Saltzman motivates the concept of gene therapy, and describes the different ways in which this is applied in different tissues. Methods to help deliver DNA into cells using viruses and cationic lipids are discussed, as a way to overcome challenges in gene therapy. Next, Professor Saltz gives a brief history of the different types of cell physiology, and how they can be affected by diseases, such as sickle cell anemia patients.</td>\n",
       "      <td>summarize: Professor Saltzman reviews the concept of gene therapy, and gives some examples of where this is applied. Methods to help deliver DNA into cells using viruses and cationic lipids are discussed, as a way to overcome some challenges in gene therapy. Next, Professor Saltzman gives a brief introduction into bacterial and mammalian cell physiology. He describes the different tissues in the body, the cell development/differentiation process, the anchorage dependence of mammalian cells that allows them to form an organism, and the extracellular matrix.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize: Three main features that Tocqueville regarded as central to American democracy are discussed: the importance of local government, the concept of â�evilleâs Cantos, and the conceptÂ of the contributive individual will. The book borrows themes from economics and sociology from film and literature from old work, and describes the evolution of the concept known as thecommune.Alexis de Tocque, Democracy in America, trans. Henry ReeveElectronic edition deposited and marked-up by ASGRP, the at the University of Virginia, June 1, 1997</td>\n",
       "      <td>summarize: Three main features that Tocqueville regarded as central to American democracy are discussed: the importance of local government, the concept of “civil association,” and “the spirit of religion.” The book is not simply a celebration of the democratic experience in America; Tocqueville is deeply worried about the potential of a democratic tyranny.Alexis de Tocqueville, Democracy in America, trans. Henry ReeveElectronic edition deposited and marked-up by ASGRP, the at the University of Virginia, June 1, 1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Generated Text  \\\n",
       "0  summarize: In this lecture, Professor Holloway revisits Malcolm Xâs life in order to offer a richer portrait of the black leader than his predecessors. A critical analysis of Malcolm X reveals that, in the absence of a strong social context, one of the most important and ultimately most pernicious debates in the Nation of Islam (NOI), is predicated on ideas of how race can be replaced by a notion of accountability. In the final year of his life, prior to his assassination by NOI members in 1965, Malcolm X makes a religious journey to Mecca, rejects his prior views on race, starts with the Organization of Afro-American Unity, and adopes the philosophy of white supremacist violence   \n",
       "1                                                                                                                                                                                                                                                 summarize: Professor Saltzman motivates the concept of gene therapy, and describes the different ways in which this is applied in different tissues. Methods to help deliver DNA into cells using viruses and cationic lipids are discussed, as a way to overcome challenges in gene therapy. Next, Professor Saltz gives a brief history of the different types of cell physiology, and how they can be affected by diseases, such as sickle cell anemia patients.   \n",
       "2                                                                                                                                                   summarize: Three main features that Tocqueville regarded as central to American democracy are discussed: the importance of local government, the concept of â�evilleâs Cantos, and the conceptÂ of the contributive individual will. The book borrows themes from economics and sociology from film and literature from old work, and describes the evolution of the concept known as thecommune.Alexis de Tocque, Democracy in America, trans. Henry ReeveElectronic edition deposited and marked-up by ASGRP, the at the University of Virginia, June 1, 1997   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Actual Text  \n",
       "0  summarize: In this lecture, Professor Holloway revisits Malcolm X’s life in order to offer a more nuanced interpretation of the black leader than is traditionally taught. Professor Holloway links Malcolm X to a tradition of black intellectuals and political activists like Booker T. Washington, Marcus Garvey, and Robert Williams, and he explores the philosophy of the Nation of Islam (NOI), the organization for which Malcolm X is the national spokesman before his split with Elijah Muhammad in 1964. In the final year of his life, prior to his assassination by NOI members in 1965, Malcolm X makes a religious journey to Mecca, rejects his prior views on race, starts the Organization of Afro-American Unity, and adopts the name  \n",
       "1                                                                                                                                                                          summarize: Professor Saltzman reviews the concept of gene therapy, and gives some examples of where this is applied. Methods to help deliver DNA into cells using viruses and cationic lipids are discussed, as a way to overcome some challenges in gene therapy. Next, Professor Saltzman gives a brief introduction into bacterial and mammalian cell physiology. He describes the different tissues in the body, the cell development/differentiation process, the anchorage dependence of mammalian cells that allows them to form an organism, and the extracellular matrix.  \n",
       "2                                                                                                                                                                                                                  summarize: Three main features that Tocqueville regarded as central to American democracy are discussed: the importance of local government, the concept of “civil association,” and “the spirit of religion.” The book is not simply a celebration of the democratic experience in America; Tocqueville is deeply worried about the potential of a democratic tyranny.Alexis de Tocqueville, Democracy in America, trans. Henry ReeveElectronic edition deposited and marked-up by ASGRP, the at the University of Virginia, June 1, 1997  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_df.style.set_properties(**{'text-align': 'left'})\n",
    "dfStyler = final_df.style.set_properties(**{'text-align': 'left'})\n",
    "dfStyler = final_df.style.set_properties(subset=['Generated Text','Actual Text'],**{'text-align': 'left'})\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "717c5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"predictions_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432cbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
