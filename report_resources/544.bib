%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Siwei Li at 2021-12-01 13:34:47 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@misc{classification,
	archiveprefix = {arXiv},
	author = {Yang Liu},
	date-added = {2021-12-01 13:16:30 -0800},
	date-modified = {2021-12-01 13:16:43 -0800},
	eprint = {1903.10318},
	primaryclass = {cs.CL},
	title = {Fine-tune BERT for Extractive Summarization},
	year = {2019}}

@article{siamese,
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	date-added = {2021-11-25 21:48:34 -0800},
	date-modified = {2021-11-25 21:49:04 -0800},
	doi = {10.1109/cvpr.2015.7298682},
	journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {Jun},
	publisher = {IEEE},
	title = {FaceNet: A unified embedding for face recognition and clustering},
	url = {http://dx.doi.org/10.1109/CVPR.2015.7298682},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1109/CVPR.2015.7298682},
	bdsk-url-2 = {http://dx.doi.org/10.1109/cvpr.2015.7298682}}

@inproceedings{combine-2020,
	abstract = {We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.},
	address = {Online},
	author = {Pilault, Jonathan and Li, Raymond and Subramanian, Sandeep and Pal, Chris},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2021-10-19 10:38:38 -0700},
	date-modified = {2021-10-19 10:38:59 -0700},
	doi = {10.18653/v1/2020.emnlp-main.748},
	month = nov,
	pages = {9308--9319},
	publisher = {Association for Computational Linguistics},
	title = {On Extractive and Abstractive Neural Document Summarization with Transformer Language Models},
	url = {https://aclanthology.org/2020.emnlp-main.748},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.748},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.748}}

@inproceedings{bottom-up,
	abstract = {Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.},
	address = {Brussels, Belgium},
	author = {Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2021-10-19 10:36:46 -0700},
	date-modified = {2021-10-19 10:37:21 -0700},
	doi = {10.18653/v1/D18-1443},
	month = oct # {-} # nov,
	pages = {4098--4109},
	publisher = {Association for Computational Linguistics},
	title = {Bottom-Up Abstractive Summarization},
	url = {https://aclanthology.org/D18-1443},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1443},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1443}}

@article{old,
	author = {Taskiran, C.M. and Pizlo, Z. and Amir, A. and Ponceleon, D. and Delp, E.J.},
	date-added = {2021-10-17 21:53:28 -0700},
	date-modified = {2021-10-17 21:54:02 -0700},
	doi = {10.1109/TMM.2006.876282},
	journal = {IEEE Transactions on Multimedia},
	number = {4},
	pages = {775-791},
	title = {Automated video program summarization using speech transcripts},
	volume = {8},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/TMM.2006.876282}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>544</string>
		<key>keys</key>
		<string>old</string>
	</dict>
</array>
</plist>
}}
